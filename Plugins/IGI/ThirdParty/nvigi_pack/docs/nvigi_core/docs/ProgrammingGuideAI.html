<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NVIGI - Programming Guide For Local And Cloud Inference &mdash; In-Game Inferencing SDK 1.0.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/pygments_dark.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/theme-switcher-general.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/omni-style-dark.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/api-styles-dark.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/api-styles.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/mermaid-init.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/theme-setter.js"></script>
        <script src="../../_static/design-tabs.js"></script>
        <script src="../../_static/version.js"></script>
        <script src="../../_static/social-media.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="NVIDIA In-Game Inference AI Plugins" href="../../plugins/sdk/README.html" />
    <link rel="prev" title="NVIGI - Programming Guide" href="ProgrammingGuide.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../../index.html">
  <img src="../../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../NVIGIDeveloperPack.html">NVIDIA In-Game Inferencing (NVIGI) Developer Pack 1.1.0 Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sample/README.html">NVIGI 3D Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="Architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="GpuSchedulingForAI.html">GPU Scheduling for AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProgrammingGuide.html">NVIGI - Programming Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">NVIGI - Programming Guide For Local And Cloud Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#version-1-1-0-release">Version 1.1.0 Release</a></li>
<li class="toctree-l2"><a class="reference internal" href="#table-of-contents">Table of Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#introduction">INTRODUCTION</a></li>
<li class="toctree-l2"><a class="reference internal" href="#key-concepts">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-repository">Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#obtaining-models">Obtaining Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#local-execution">Local Execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#prompt-templates-for-llm-models">Prompt Templates for LLM Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#remote-execution">Remote Execution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#common-and-custom-capabilities-and-requirements">Common And Custom Capabilities and Requirements</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#local-plugins">Local Plugins</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cloud-plugins">Cloud Plugins</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#creation-parameters">Creation Parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-and-custom">Common and Custom</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Cloud Plugins</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Local Plugins</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#compute-in-graphics-cig">Compute In Graphics (CIG)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-types">Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="#input-slots">Input Slots</a></li>
<li class="toctree-l2"><a class="reference internal" href="#execution-context">Execution Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#blocking-vs-asynchronous-evaluation">Blocking Vs Asynchronous Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#obtaining-results">Obtaining Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#callback-approach">Callback Approach</a></li>
<li class="toctree-l3"><a class="reference internal" href="#polling-approach">Polling Approach</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins/sdk/README.html">NVIDIA In-Game Inference AI Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins/sdk/docs/ProgrammingGuideASRWhisper.html">Automatic Speech Recognition (ASR) - Whisper Programming Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins/sdk/docs/ProgrammingGuideGPT.html">Generative Pre-Trained Transformers (GPT) Programming Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins/sdk/docs/ProgrammingGuideEmbed.html">Embedding (EMBED) Programming Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../source-build/README.html">NVIGI Public Source GitHub Pull-and-Build Scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins/sdk/docs/CustomizingPlugins.html">Creating a Customized Plugin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins/sdk/3rd-party-licenses.html">3rd PARTY SOFTWARE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">In-Game Inferencing SDK</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">


<li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
  
<li>NVIGI - Programming Guide For Local And Cloud Inference</li>

      <li class="wy-breadcrumbs-aside">
      </li>
<li class="wy-breadcrumbs-aside">

  <span>&nbsp;</span>
</li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nvigi-programming-guide-for-local-and-cloud-inference">
<h1>NVIGI - Programming Guide For Local And Cloud Inference<a class="headerlink" href="#nvigi-programming-guide-for-local-and-cloud-inference" title="Permalink to this headline"></a></h1>
<p>This guide primarily focuses on the general use of the AI plugins performing local or cloud inference. Before reading this please read <a class="reference internal" href="ProgrammingGuide.html"><span class="doc std std-doc">the general programming guide</span></a></p>
<blockquote>
<div><p><strong>IMPORTANT</strong>: This guide might contain pseudo code, for the up to date implementation and source code which can be copy pasted please see the <span class="xref myst">basic sample</span></p>
</div></blockquote>
<section id="version-1-1-0-release">
<h2>Version 1.1.0 Release<a class="headerlink" href="#version-1-1-0-release" title="Permalink to this headline"></a></h2>
</section>
<section id="table-of-contents">
<h2>Table of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#introduction"><span class="std std-doc">Introduction</span></a></p></li>
<li><p><a class="reference internal" href="#key-concepts"><span class="std std-doc">Key Concepts</span></a></p></li>
<li><p><a class="reference internal" href="#model-repository"><span class="std std-doc">Model Repository</span></a></p></li>
<li><p><a class="reference internal" href="#obtaining-models"><span class="std std-doc">Obtaining Models</span></a></p>
<ul>
<li><p><a class="reference internal" href="#local-execution"><span class="std std-doc">Local Execution</span></a></p></li>
<li><p><a class="reference internal" href="#remote-execution"><span class="std std-doc">Remote Execution</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#common-and-custom-capabilities-and-requirements"><span class="std std-doc">Obtaining Plugin Capabilities and Requirements</span></a></p></li>
<li><p><a class="reference internal" href="#creation-parameters"><span class="std std-doc">Creation Parameters</span></a></p>
<ul>
<li><p><a class="reference internal" href="#common-and-custom"><span class="std std-doc">Common and Custom</span></a></p></li>
<li><p><a class="reference internal" href="#cloud-plugins"><span class="std std-doc">Cloud Plugins</span></a></p></li>
<li><p><a class="reference internal" href="#local-plugins"><span class="std std-doc">Local Plugins</span></a></p></li>
<li><p><a class="reference internal" href="#compute-in-graphics-cig"><span class="std std-doc">Compute In Graphics (CIG)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#data-types"><span class="std std-doc">Data Types</span></a></p></li>
<li><p><a class="reference internal" href="#input-slots"><span class="std std-doc">Inputs Slots</span></a></p></li>
<li><p><a class="reference internal" href="#execution-context"><span class="std std-doc">Execution Context</span></a></p>
<ul>
<li><p><a class="reference internal" href="#blocking-vs-asynchronous-evaluation"><span class="std std-doc">Synchronous vs Asynchronous Execution</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#obtaining-results"><span class="std std-doc">Obtaining Results</span></a></p>
<ul>
<li><p><a class="reference internal" href="#callback-approach"><span class="std std-doc">Callback Approach</span></a></p></li>
<li><p><a class="reference internal" href="#polling-approach"><span class="std std-doc">Polling Approach</span></a></p></li>
</ul>
</li>
</ul>
</section>
<section id="introduction">
<h2>INTRODUCTION<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>NVIGI AI plugins provide unified API for both local and cloud inference. This ensures easy transition between local and cloud services and full flexibility. The AI API is located in <code class="docutils literal notranslate"><span class="pre">nvigi_ai.h</span></code> header.</p>
</section>
<section id="key-concepts">
<h2>Key Concepts<a class="headerlink" href="#key-concepts" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Each AI plugin implements certain feature with the specific backend and underlying API, here are some examples:</p>
<ul>
<li><p>nvigi.plugin.gpt.ggml.cuda -&gt; implements GPT feature using <strong>GGML backend and CUDA API for local execution</strong></p></li>
<li><p>nvigi.plugin.gpt.cloud.rest -&gt; implements GPT feature using <strong>CLOUD backed and REST API for remote execution</strong></p></li>
</ul>
</li>
<li><p>Models used by the AI <strong>local</strong> plugins are stored in a specific NVIGI model repository (more details in sections below)</p></li>
<li><p>All AI plugins implement and export the same generic interface <code class="docutils literal notranslate"><span class="pre">InferenceInterface</span></code></p></li>
<li><p>AI plugins can act as “parent” plugins encapsulating multiple features but still exposing the same unified API</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">InferenceInterface</span></code> is used to obtain capabilities and requirements (VRAM etc.) and also create and destroy instance(s)</p></li>
<li><p>Each created instance is represented by the <code class="docutils literal notranslate"><span class="pre">InferenceInstance</span></code> interface</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">InferenceInstance</span></code> contains generic API for running the inference given the <code class="docutils literal notranslate"><span class="pre">InferenceExecutionContext</span></code> which contains input slots, callbacks to get results, runtime parameters etc.</p></li>
<li><p>All inputs and outputs use generic <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">slots</span></code>, like for example <code class="docutils literal notranslate"><span class="pre">InferenceDataByteArray</span></code>, <code class="docutils literal notranslate"><span class="pre">InferenceDataText</span></code>, <code class="docutils literal notranslate"><span class="pre">InferenceDataAudio</span></code> etc.</p></li>
<li><p>Host application obtains results (output slots) either via registered callback or polling mechanism</p></li>
</ul>
</section>
<section id="model-repository">
<h2>Model Repository<a class="headerlink" href="#model-repository" title="Permalink to this headline"></a></h2>
<p>For consistency, all NVIGI AI inference plugins store their models using the following directory structure:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ROOT/
├── nvigi.plugin.$name.$backend
    └── {MODEL_GUID}
        └── files
</pre></div>
</div>
<p>Here is an example structure for the existing NVIGI plugins and models:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ROOT/
├── nvigi.plugin.gpt.ggml
│   └── {175C5C5D-E978-41AF-8F11-880D0517C524}
│       ├── gpt-7b-chat-q4.gguf
│       └── nvigi.model.config.json
└── nvigi.plugin.asr.ggml
    └── {5CAD3A03-1272-4D43-9F3D-655417526170}
        ├── ggml-asr-small.gguf
        └── nvigi.model.config.json
</pre></div>
</div>
<blockquote>
<div><p>NOTE: Each plugin can have as many different models (GUIDs) as needed</p>
</div></blockquote>
<p>Each model must provide <code class="docutils literal notranslate"><span class="pre">nvigi.model.config.json</span></code> file containing:</p>
<ul class="simple">
<li><p>model’s card (name, file extension(s) and instructions on how to obtain it)</p></li>
<li><p>vram consumption (if local)</p></li>
<li><p>other model specific information (e.g. prompt template for LLM models)</p></li>
</ul>
<p>Here is an example from <code class="docutils literal notranslate"><span class="pre">nvigi.plugin.gpt.ggml</span></code>, model information for <code class="docutils literal notranslate"><span class="pre">llama3.1</span> <span class="pre">8B</span> <span class="pre">instruct</span></code>:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;llama-3.1-8b-instruct&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;vram&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5124</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;prompt_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;&lt;|begin_of_text|&gt;&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$system&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$user&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$assistant&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">],</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;turn_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$user&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">],</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;ext&quot;</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gguf&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;notes&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Must use .gguf extension and format, model(s) can be obtained for free on huggingface&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;file&quot;</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nt">&quot;command&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;curl -L -o llama-3.1-8b-instruct.gguf &#39;https://huggingface.co/ArtyLLaMa/LLaMa3.1-Instruct-8b-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf?download=true&#39;&quot;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p>NOTE: Some models will only be accessible via NGC and require special token for access. These models are normally licensed differently and require developers to contact NVIDIA to obtain access.</p>
</div></blockquote>
<p>An optional <code class="docutils literal notranslate"><span class="pre">configs</span></code> subfolder, with the identical subfolder structure, can be added under <code class="docutils literal notranslate"><span class="pre">$ROOT</span></code> to provide <code class="docutils literal notranslate"><span class="pre">nvigi.model.config.json</span></code> overrides as shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ROOT/
├── configs
    ├── nvigi.plugin.$name.$backend
    └── {MODEL_GUID}
        ├── nvigi.model.config.json
        | etc
├── nvigi.plugin.$name.$backend
    └── {MODEL_GUID}
        ├── files        
</pre></div>
</div>
<blockquote>
<div><p>NOTE:
This allows quick way of modifying only model config settings in JSON without having to reupload the entire model which can be several GBs in size</p>
</div></blockquote>
</section>
<section id="obtaining-models">
<h2>Obtaining Models<a class="headerlink" href="#obtaining-models" title="Permalink to this headline"></a></h2>
<section id="local-execution">
<h3>Local Execution<a class="headerlink" href="#local-execution" title="Permalink to this headline"></a></h3>
<p>As mentioned in the above section, each model configuration JSON contains model card with instructions on how to obtain each model.</p>
<p>To add new local model to the model repository please follow these steps:</p>
<ul class="simple">
<li><p>Generate new GUID in the <code class="docutils literal notranslate"><span class="pre">registry</span> <span class="pre">format</span></code>, like for example <code class="docutils literal notranslate"><span class="pre">{2467C733-2936-4187-B7EE-B53C145288F3}</span></code></p></li>
<li><p>Create new folder under <code class="docutils literal notranslate"><span class="pre">nvigi.plugins.$feature.$backend</span></code> and name it the above GUID (like for example <code class="docutils literal notranslate"><span class="pre">{2467C733-2936-4187-B7EE-B53C145288F3}</span></code>)</p></li>
<li><p>Copy an existing <code class="docutils literal notranslate"><span class="pre">nvigi.model.config.json</span></code> from already available models</p></li>
<li><p>Modify <code class="docutils literal notranslate"><span class="pre">name</span></code> field in the JSON to match your model</p></li>
<li><p>Modify <code class="docutils literal notranslate"><span class="pre">vram</span></code> field to match your model’s VRAM requirements in MB (NVIGI logs VRAM consumption per instance in release/debug build configurations)</p></li>
<li><p>Modify any custom model specific bits (for example, each GPT/LLM model requires specific prompt setup)</p></li>
<li><p>Download model from Hugging Face or other source (NGC etc.)</p></li>
<li><p>Unzip any archives and ensure correct extension(s) are used (for example if using GGML backend all models must use <code class="docutils literal notranslate"><span class="pre">.gguf</span></code> extension)</p></li>
</ul>
<blockquote>
<div><p>NOTE: Please keep in mind that <strong>some latest models might not work</strong> with the backends provided in this version of NVIGI SDK hence plugin(s) need to be upgraded.</p>
</div></blockquote>
<section id="prompt-templates-for-llm-models">
<h4>Prompt Templates for LLM Models<a class="headerlink" href="#prompt-templates-for-llm-models" title="Permalink to this headline"></a></h4>
<p>Each LLM required correct prompt template. These templates are stored in the above mentioned model configuration JSON and look something like this:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="nt">&quot;prompt_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;&lt;|begin_of_text|&gt;&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$system&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$user&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$assistant&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">],</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;turn_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;$user&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s2">&quot;\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">],</span><span class="w"></span>
</pre></div>
</div>
<p>To make these prompt templates, one needs to find the jinja (very simplistic programming language) that the model uses to format itself (or more trivially, look for “<model name> prompt template” in google), and see what it wants for the various markers.
For example, SmolLM2, we find this: https://ollama.com/library/smollm2:360m/blobs/d502d55c1d60</p>
<p>Next step is to find where the system, user, and assistant start and stop markers are</p>
<div class="highlight-txt notranslate"><div class="highlight"><pre><span></span>{{- if .System }}&lt;|im_start|&gt;system
{{ .System }}&lt;|im_end|&gt;
{{- if eq .Role &quot;user&quot; }}&lt;|im_start|&gt;user
{{ .Content }}&lt;|im_end|&gt;
{{ else if eq .Role &quot;assistant&quot; }}&lt;|im_start|&gt;assistant
{{ .Content }}{{ if not $last }}&lt;|im_end|&gt;
</pre></div>
</div>
<p>Now we know the LLM wants something that looks like this:</p>
<div class="highlight-txt notranslate"><div class="highlight"><pre><span></span>&lt;|im_start|&gt;system
You are a helpful Ai agent...
&lt;|im_end|&gt;
&lt;|im_start|&gt;user
Can you tell me a story to help my daughter go to sleep?
&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
Sure, Once upon a time...
&lt;|im_end|&gt;
</pre></div>
</div>
<p>Hence we make the prompt template like this</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="nt">&quot;prompt_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;&lt;|im_start|&gt;system\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;$system&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;&lt;|im_end|&gt;\n\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;&lt;|im_start|&gt;user\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;$user&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;$assistant&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">],</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p>NOTE: IGI does NOT automatically add newlines, special care must be taken to include <code class="docutils literal notranslate"><span class="pre">\n</span></code> correctly</p>
</div></blockquote>
<p>IGI uses the “prompt_template” on either the <em>first</em> chat message or if we’re using the LLM in instruct mode (no chat history). For the chat mode, the “turn_template” is required for all remaining turns.
This ensures that the last assistant message is terminated properly (if necessary, sometimes the model auto puts this in), and then replicating the user/assistant turn from the prompt template.</p>
<p>So turn template for SMOLLM 2 becomes</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="nt">&quot;turn_template&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;&lt;|im_start|&gt;user\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;$user&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="s2">&quot;$assistant&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p">]</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>
<section id="remote-execution">
<h3>Remote Execution<a class="headerlink" href="#remote-execution" title="Permalink to this headline"></a></h3>
<p>To add new remote (cloud) model to the model repository please follow these steps:</p>
<ul class="simple">
<li><p>Generate new GUID in the <code class="docutils literal notranslate"><span class="pre">registry</span> <span class="pre">format</span></code>, like for example <code class="docutils literal notranslate"><span class="pre">{2467C733-2936-4187-B7EE-B53C145288F3}</span></code></p></li>
<li><p>Create new folder under <code class="docutils literal notranslate"><span class="pre">nvigi.plugins.gpt.cloud</span></code> and name it the above GUID (like for example <code class="docutils literal notranslate"><span class="pre">{2467C733-2936-4187-B7EE-B53C145288F3}</span></code>)</p></li>
<li><p>Copy an existing <code class="docutils literal notranslate"><span class="pre">nvigi.model.config.json</span></code> from already available models (for example from <code class="docutils literal notranslate"><span class="pre">model/llama-3.2-3b</span></code> located at <code class="docutils literal notranslate"><span class="pre">$ROOT\nvigi.plugin.gpt.cloud\{01F43B70-CE23-42CA-9606-74E80C5ED0B6}\nvigi.model.config.json</span></code>)</p></li>
<li><p>Modify <code class="docutils literal notranslate"><span class="pre">name</span></code> field in the JSON to match your model</p></li>
<li><p>Modify <code class="docutils literal notranslate"><span class="pre">request_body</span></code> field to match your model’s JSON body for the REST request</p></li>
</ul>
<p>If using <a class="reference external" href="https://build.nvidia.com">NVIDIA NIM APIs</a> search and navigate to the model you want to use then copy paste request into the above mentioned JSON. For example, when selecting <a class="reference external" href="https://build.nvidia.com/meta/llama-3_1-70b-instruct">llama-3_1-70b-instruct</a> the completion code in Python looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta/llama-3.1-70b-instruct&quot;</span><span class="p">,</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;Write a limerick about the wonders of GPU computing.&quot;</span><span class="p">}],</span>
  <span class="n">temperature</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
  <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
  <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
  <span class="n">stream</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># NOTE: This is NOT supported by the current version of GPT cloud plugin so it must be set to false (see below)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>which then translates to the <code class="docutils literal notranslate"><span class="pre">nvigi.model.config.json</span></code> file looking like this:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;llama-3.1-70b-instruct&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;vram&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;request_body&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;meta/llama-3.1-70b-instruct&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;$system&quot;</span><span class="p">},</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;$user&quot;</span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">],</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;top_p&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.7</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;max_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1024</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;stream&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p>IMPORTANT: Current version of <code class="docutils literal notranslate"><span class="pre">nvigi.plugin.gpt.cloud.rest</span></code> does not support cloud streaming so that option needs to be set to <code class="docutils literal notranslate"><span class="pre">false</span></code></p>
</div></blockquote>
<p>For 3rd party cloud solutions, like for example OpenAI, please have a look at the model with GUID <code class="docutils literal notranslate"><span class="pre">{E9102ACB-8CD8-4345-BCBF-CCF6DC758E58}</span></code> which contains configuration for <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code>. This model can be used the exact same way as any other NIMS based model provided by NVIDIA and also can be used as a template to clone other models which are based on the OpenAI API (just don’t forget to generate new GUID). Here is the config file:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;openai/gpt-3.5-turbo&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;vram&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="nt">&quot;request_body&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;$system&quot;</span><span class="p">},</span><span class="w"></span>
<span class="w">      </span><span class="p">{</span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;$user&quot;</span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">],</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;top_p&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.7</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;max_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1024</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;stream&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p>IMPORTANT: If your custom model does not follow the OpenAI API standard for the REST cloud requests, the <code class="docutils literal notranslate"><span class="pre">request_body</span></code> section must be modified to match specific server requirements.</p>
</div></blockquote>
</section>
</section>
<section id="common-and-custom-capabilities-and-requirements">
<h2>Common And Custom Capabilities and Requirements<a class="headerlink" href="#common-and-custom-capabilities-and-requirements" title="Permalink to this headline"></a></h2>
<blockquote>
<div><p><strong>IMPORTANT NOTE</strong>: This section covers a scenario where the host application can instantiate models that were not included when the application was packaged and shipped. If the models and their capabilities are predefined and there is no need for dynamically downloaded models, you can skip to the next section.</p>
</div></blockquote>
<p>If host application needs to find out more information about the available models in the above mentioned repository, the <code class="docutils literal notranslate"><span class="pre">InferenceInterface</span></code> provides <code class="docutils literal notranslate"><span class="pre">getCapsAndRequirements</span></code> API which returns feature specific (if any) caps and requirements and common caps and requirements shown below:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Generic caps and requirements - apply to all plugins</span>
<span class="c1">//! </span>
<span class="c1">//! {1213844E-E53B-4C46-A303-741789060B3C}</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">alignas</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="w"> </span><span class="n">CommonCapabilitiesAndRequirements</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">CommonCapabilitiesAndRequirements</span><span class="p">()</span><span class="w"> </span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">NVIGI_UID</span><span class="p">(</span><span class="n">UID</span><span class="p">({</span><span class="w"> </span><span class="mh">0x1213844e</span><span class="p">,</span><span class="w"> </span><span class="mh">0xe53b</span><span class="p">,</span><span class="w"> </span><span class="mh">0x4c46</span><span class="p">,{</span><span class="w"> </span><span class="mh">0xa3</span><span class="p">,</span><span class="w"> </span><span class="mh">0x3</span><span class="p">,</span><span class="w"> </span><span class="mh">0x74</span><span class="p">,</span><span class="w"> </span><span class="mh">0x17</span><span class="p">,</span><span class="w"> </span><span class="mh">0x89</span><span class="p">,</span><span class="w"> </span><span class="mh">0x6</span><span class="p">,</span><span class="w"> </span><span class="mh">0xb</span><span class="p">,</span><span class="w"> </span><span class="mh">0x3c</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">}),</span><span class="w"> </span><span class="n">kStructVersion1</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">numSupportedModels</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">supportedModelGUIDs</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">supportedModelNames</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="kt">size_t</span><span class="o">*</span><span class="w"> </span><span class="n">modelMemoryBudgetMB</span><span class="p">{};</span><span class="w"> </span><span class="c1">//! IMPORTANT: Provided if known, can be 0 if fully dynamic and depends on inputs</span>
<span class="w">    </span><span class="n">InferenceBackendLocations</span><span class="w"> </span><span class="n">supportedBackends</span><span class="p">{};</span><span class="w"></span>

<span class="w">    </span><span class="c1">//! NEW MEMBERS GO HERE, BUMP THE VERSION!</span>
<span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<p>Each AI interface provides a generic API (located in <code class="docutils literal notranslate"><span class="pre">nvigi_ai.h</span></code>) used to obtain <code class="docutils literal notranslate"><span class="pre">CommonCapabilitiesAndRequirements</span></code> and any custom capabilities and requirements (if any).</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Returns model information</span>
<span class="c1">//!</span>
<span class="c1">//! Call this method to find out about the available models and their capabilities and requirements.</span>
<span class="c1">//! </span>
<span class="c1">//! @param modelInfo Pointer to a structure containing supported model information</span>
<span class="c1">//! @param params Optional pointer to the setup parameters (can be null)</span>
<span class="c1">//! @return nvigi::kResultOk if successful, error code otherwise (see NVIGI_result.h for details)</span>
<span class="c1">//!</span>
<span class="c1">//! NOTE: It is recommended to use the templated &#39;getCapsAndRequirements&#39; helper (see below in this header).</span>
<span class="c1">//! </span>
<span class="c1">//! This method is NOT thread safe.</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">Result</span><span class="p">(</span><span class="o">*</span><span class="n">getCapsAndRequirements</span><span class="p">)(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">NVIGIParameter</span><span class="o">**</span><span class="w"> </span><span class="n">modelInfo</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">NVIGIParameter</span><span class="o">*</span><span class="w"> </span><span class="n">params</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>When obtaining information about specific model(s) host application can provide <code class="docutils literal notranslate"><span class="pre">CommonCreationParameters</span></code> (see next section for more details) as input so there are several options based on selected backend etc.</p>
<section id="local-plugins">
<h3>Local Plugins<a class="headerlink" href="#local-plugins" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>provide specific model GUID and VRAM budget and check if that particular model can run within the budget</p></li>
<li><p>provide null model GUID and VRAM budget to get a list of models that can run within the budget</p></li>
<li><p>provide null model GUID and “infinite” (MAX_INT) VRAM budget to get a list of ALL models</p></li>
</ul>
</section>
<section id="cloud-plugins">
<h3>Cloud Plugins<a class="headerlink" href="#cloud-plugins" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>provide specific model GUID to obtain CloudCapabilities which include URL and other information for the endpoint used by the model</p></li>
<li><p>provide null model GUID to get a list of ALL models (CloudCapabilities in this case will NOT provide any info)</p></li>
</ul>
<p>If specific feature has custom capabilities and requirements the common ones will always be either chained together or returned as a pointer within the custom caps. In addition, if feature is a pipeline (parent plugin encapsulating two or more plugins) it will return caps and requirements for ALL enclosed plugins.  These can be queryied from what is returned by the parent plugin’s <code class="docutils literal notranslate"><span class="pre">getCapsAndRequirements</span></code> using <code class="docutils literal notranslate"><span class="pre">nvigi::findStruct&lt;structtype&gt;(rootstruct)</span></code>.</p>
<blockquote>
<div><p>IMPORTANT: Always have a look at plugin’s public header <code class="docutils literal notranslate"><span class="pre">nvigi_$feature.h</span></code> to find out if plugin has custom caps etc.</p>
</div></blockquote>
</section>
</section>
<section id="creation-parameters">
<h2>Creation Parameters<a class="headerlink" href="#creation-parameters" title="Permalink to this headline"></a></h2>
<section id="common-and-custom">
<h3>Common and Custom<a class="headerlink" href="#common-and-custom" title="Permalink to this headline"></a></h3>
<p>Before creating an instance, host application must specify certain properties like which model should be used, how much VRAM is available, how many CPU threads etc. Similar to the previous section, plugins can have custom creation parameters but they always have to provide common ones. Here is how common creation parameters look like:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Generic creation parameters - apply to all plugins</span>
<span class="c1">//! </span>
<span class="c1">//! {CC8CAD78-95F0-41B0-AD9C-5D6995988B23}</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">alignas</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="w"> </span><span class="n">CommonCreationParameters</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">CommonCreationParameters</span><span class="p">()</span><span class="w"> </span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">NVIGI_UID</span><span class="p">(</span><span class="n">UID</span><span class="p">({</span><span class="w"> </span><span class="mh">0xcc8cad78</span><span class="p">,</span><span class="w"> </span><span class="mh">0x95f0</span><span class="p">,</span><span class="w"> </span><span class="mh">0x41b0</span><span class="p">,{</span><span class="w"> </span><span class="mh">0xad</span><span class="p">,</span><span class="w"> </span><span class="mh">0x9c</span><span class="p">,</span><span class="w"> </span><span class="mh">0x5d</span><span class="p">,</span><span class="w"> </span><span class="mh">0x69</span><span class="p">,</span><span class="w"> </span><span class="mh">0x95</span><span class="p">,</span><span class="w"> </span><span class="mh">0x98</span><span class="p">,</span><span class="w"> </span><span class="mh">0x8b</span><span class="p">,</span><span class="w"> </span><span class="mh">0x23</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">}),</span><span class="w"> </span><span class="n">kStructVersion1</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">numThreads</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">vramBudgetMB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SIZE_MAX</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">modelGUID</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">utf8PathToModels</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="c1">//! Optional - additional models downloaded on the system (if any)</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">utf8PathToAdditionalModels</span><span class="p">{};</span><span class="w"></span>

<span class="w">    </span><span class="c1">//! NEW MEMBERS GO HERE, BUMP THE VERSION!</span>
<span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<p>Plugins cannot create an instance unless they know where NVIGI models repository is located, what model GUID to use, how much VRAM is OK to use etc. All this information is provided in common creation parameters. Each plugin can define custom ones, this obviously depends on what parameters are needed to create an instance.</p>
<blockquote>
<div><p>NOTE:
Same model GUID can be used by different plugins if they are implementing different backends, for example Whisper GGUF model can be loaded by the <code class="docutils literal notranslate"><span class="pre">nvigi.plugin.asr.ggml.cuda</span></code> and <code class="docutils literal notranslate"><span class="pre">nvigi.plugin.asr.ggml.cpu</span></code> plugins</p>
</div></blockquote>
</section>
<section id="id1">
<h3>Cloud Plugins<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>When it comes to cloud plugins they can use two different protocols, REST and gRPC. In addition to the above mentioned common creation parameters cloud plugin require either <code class="docutils literal notranslate"><span class="pre">RESTParameters</span></code> or <code class="docutils literal notranslate"><span class="pre">RPCParameters</span></code> to be chained together with the common ones. Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Obtain GPT CLOUD REST interface</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">IGeneralPurposeTransformer</span><span class="o">*</span><span class="w"> </span><span class="n">igpt</span><span class="p">{};</span><span class="w"></span>
<span class="n">nvigiGetInterface</span><span class="p">(</span><span class="n">plugin</span><span class="o">::</span><span class="n">gpt</span><span class="o">::</span><span class="n">cloud</span><span class="o">::</span><span class="n">rest</span><span class="o">::</span><span class="n">kId</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">igpt</span><span class="p">);</span><span class="w"></span>

<span class="c1">//! Common parameters</span>
<span class="n">CommonCreationParameters</span><span class="w"> </span><span class="n">common</span><span class="p">{};</span><span class="w"></span>
<span class="n">common</span><span class="p">.</span><span class="n">utf8PathToModels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">.</span><span class="n">modelDir</span><span class="p">.</span><span class="n">c_str</span><span class="p">();</span><span class="w"></span>
<span class="n">common</span><span class="p">.</span><span class="n">modelGUID</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;{E9102ACB-8CD8-4345-BCBF-CCF6DC758E58}&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// gpt-3.5-turbo</span>

<span class="c1">//! GPT parameters</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">GPTCreationParameters</span><span class="w"> </span><span class="n">gptCreationParams</span><span class="p">{};</span><span class="w"></span>
<span class="c1">// TODO: Set some GPT specific items here</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">gptCreationParams</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">common</span><span class="p">)))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="c1">// Handle error</span>
<span class="p">}</span><span class="w"></span>

<span class="c1">//! Cloud parameters</span>
<span class="n">RESTParameters</span><span class="w"> </span><span class="n">cloudParams</span><span class="p">{};</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">token</span><span class="p">;</span><span class="w"></span>
<span class="n">getEnvVar</span><span class="p">(</span><span class="s">&quot;OPENAI_TOKEN&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">token</span><span class="p">);</span><span class="w"></span>
<span class="n">cloudParams</span><span class="p">.</span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;https://api.openai.com/v1/chat/completions&quot;</span><span class="p">;</span><span class="w"></span>
<span class="n">cloudParams</span><span class="p">.</span><span class="n">authenticationToken</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">token</span><span class="p">.</span><span class="n">c_str</span><span class="p">();</span><span class="w"></span>
<span class="n">cloudParams</span><span class="p">.</span><span class="n">verboseMode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w"></span>

<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">gptCreationParams</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">cloudParams</span><span class="p">)))</span><span class="w"> </span><span class="c1">// Chaining cloud parameters!</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="c1">// Handle error</span>
<span class="p">}</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceInstance</span><span class="o">*</span><span class="w"> </span><span class="n">instance</span><span class="p">{};</span><span class="w"></span>
<span class="n">igpt</span><span class="o">-&gt;</span><span class="n">createInstance</span><span class="p">(</span><span class="n">gptCreationParams</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">instance</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Local Plugins<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<p>With local plugins there are few key points to consider when selecting which plugin to use:</p>
<ul class="simple">
<li><p>Selecting backend and API</p></li>
<li><p>How much VRAM can be used?</p></li>
<li><p>What is the expected latency?</p></li>
</ul>
<p>For example fully GPU bottle-necked application or application which does not have enough VRAM left could do the following and run GPT inference completely on the CPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Obtain GPT GGML CPU interface</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">IGeneralPurposeTransformer</span><span class="o">*</span><span class="w"> </span><span class="n">igpt</span><span class="p">{};</span><span class="w"></span>
<span class="n">nvigiGetInterface</span><span class="p">(</span><span class="n">plugin</span><span class="o">::</span><span class="n">gpt</span><span class="o">::</span><span class="n">ggml</span><span class="o">::</span><span class="n">cpu</span><span class="o">::</span><span class="n">kId</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">igpt</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>On the other hand, CPU bottle-necked application could do the following and run GPT inference completely on the GPU:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Obtain GPT GGML CUDA interface</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">IGeneralPurposeTransformer</span><span class="o">*</span><span class="w"> </span><span class="n">igpt</span><span class="p">{};</span><span class="w"></span>
<span class="n">nvigiGetInterface</span><span class="p">(</span><span class="n">plugin</span><span class="o">::</span><span class="n">gpt</span><span class="o">::</span><span class="n">ggml</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">kId</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">igpt</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<section id="compute-in-graphics-cig">
<h4>Compute In Graphics (CIG)<a class="headerlink" href="#compute-in-graphics-cig" title="Permalink to this headline"></a></h4>
<p>When selecting <strong>local inference plugins which utilize CUDA API</strong> it is <strong>essential to enable CIG if application is using D3D12 or Vulkan rendering APIs</strong>. This ensures optimal performance and minimizes latency for local inference execution. CIG is enabled via special interface and here are the steps:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Obtain special HW interface for CUDA</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">IHWICuda</span><span class="o">*</span><span class="w"> </span><span class="n">icig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"></span>
<span class="n">nvigiGetInterface</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">plugin</span><span class="o">::</span><span class="n">hwi</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">kId</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">icig</span><span class="p">);</span><span class="w"></span>

<span class="c1">// Specify your device and queue information</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">D3D12Parameters</span><span class="w"> </span><span class="n">d3d12Params</span><span class="p">;</span><span class="w"></span>
<span class="n">d3d12Params</span><span class="p">.</span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&lt;</span><span class="n">your</span><span class="w"> </span><span class="n">ID3D12Device</span><span class="o">*&gt;</span><span class="w"></span>
<span class="n">d3d12Params</span><span class="p">.</span><span class="n">queue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&lt;</span><span class="n">your</span><span class="w"> </span><span class="p">(</span><span class="n">graphics</span><span class="p">)</span><span class="w"> </span><span class="n">ID3D12CommandQueue</span><span class="o">*&gt;</span><span class="w"></span>

<span class="c1">// Chain the D3D12 parameters to any creation parameters when generating local instance</span>

<span class="c1">// For example, local GPT using GGML backed and CUDA API (NOT CPU)</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">gptCreationParams</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">d3d12Parameters</span><span class="p">)))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="c1">// Handle error</span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="data-types">
<h2>Data Types<a class="headerlink" href="#data-types" title="Permalink to this headline"></a></h2>
<p>Here are the common inference data types as declared in <code class="docutils literal notranslate"><span class="pre">nvigi_ai.h</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">InferenceDataText</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">InferenceDataAudio</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">InferenceDataTextByteArray</span></code></p></li>
</ul>
<p>The underlying raw data can be either on the CPU or GPU so it is represented by the types declared in <code class="docutils literal notranslate"><span class="pre">nvigi_cpu.h</span></code>, <code class="docutils literal notranslate"><span class="pre">nvigi_d3d12.h</span></code>, <code class="docutils literal notranslate"><span class="pre">nvigi_vulkan.h</span></code> etc.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">CpuData</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">D3D12Data</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VulkanData</span></code></p></li>
</ul>
<p>For example, this is how one would setup some audio data located on the CPU using the STL helpers from <code class="docutils literal notranslate"><span class="pre">nvigi_stl_helpers.h</span></code></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">int16</span><span class="o">&gt;</span><span class="w"> </span><span class="n">my_audio</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">recordMyMonoAudio</span><span class="p">();</span><span class="w"></span>
<span class="c1">// Auto convert to the underlying `InferenceDataAudio`, single channel, PCM16</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataAudioSTLHelper</span><span class="w"> </span><span class="nf">audioData</span><span class="p">(</span><span class="n">my_audio</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Another example, this time setting up a prompt for the GPT plugin:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Hello World!&quot;</span><span class="p">;</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataTextSTLHelper</span><span class="w"> </span><span class="nf">userPrompt</span><span class="p">(</span><span class="n">text</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="input-slots">
<h2>Input Slots<a class="headerlink" href="#input-slots" title="Permalink to this headline"></a></h2>
<p>Once we have our instance we need to provide input data slots that match the input signature for the given instance. The <code class="docutils literal notranslate"><span class="pre">InferenceInstance</span></code> provides and API to obtain input and output signatures at runtime but they can also be obtained from plugin’s headers and source code. In this guide we will use the Automated Speech Recognition (ASR) as an example.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Audio data slot is coming from our previous step, note that we are using operator to convert audioData to InferenceDataAudio*</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlot</span><span class="o">&gt;</span><span class="w"> </span><span class="n">slots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kASRDataSlotAudio</span><span class="p">,</span><span class="w"> </span><span class="n">audioData</span><span class="p">}</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlotArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="p">};</span><span class="w"> </span><span class="c1">// Input slots</span>
</pre></div>
</div>
<blockquote>
<div><p>NOTE: STL helpers provide an operator which automatically converts data to the underlying low level type used by NVIGI</p>
</div></blockquote>
</section>
<section id="execution-context">
<h2>Execution Context<a class="headerlink" href="#execution-context" title="Permalink to this headline"></a></h2>
<p>Before instance can be evaluated the <code class="docutils literal notranslate"><span class="pre">InferenceExectionContext</span></code> must be created and populated with all the necessary information. This context contains:</p>
<ul class="simple">
<li><p>pointer to the instance to use</p></li>
<li><p>pointer to the optional callback to receive results</p></li>
<li><p>pointer to the optional context for the above callback</p></li>
<li><p>pointer to input data slots</p></li>
<li><p>pointer to the output data slots (optional and normally provided by plugins)</p></li>
<li><p>pointer to any runtime parameters (again can be chained together as needed)</p></li>
</ul>
<p>The following sections contain examples showing how to utilize execution context.</p>
<section id="blocking-vs-asynchronous-evaluation">
<h3>Blocking Vs Asynchronous Evaluation<a class="headerlink" href="#blocking-vs-asynchronous-evaluation" title="Permalink to this headline"></a></h3>
<p>Each plugin can opt to implement blocking and/or non-blocking API used to evaluate instance (essentially run an inference pass). For example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionContext</span><span class="w"> </span><span class="n">ctx</span><span class="p">{};</span><span class="w"></span>
<span class="n">ctx</span><span class="p">.</span><span class="n">runtimeParameters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runtime</span><span class="p">;</span><span class="w"></span>
<span class="n">ctx</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">instance</span><span class="p">;</span><span class="w"></span>
<span class="n">ctx</span><span class="p">.</span><span class="n">callback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myCallback</span><span class="p">;</span><span class="w"> </span><span class="c1">// called on a different thread</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">callbackUserData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">myCtx</span><span class="p">;</span><span class="w"></span>
<span class="n">ctx</span><span class="p">.</span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Async approach, returns immediately, <strong>callback is triggered from a different thread managed by the instance</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluateAsync</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">)</span><span class="w"></span>
</pre></div>
</div>
<p>Blocking approach, returns when done, <strong>callback is triggered on this thread</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ctx</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">)</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>
<section id="obtaining-results">
<h2>Obtaining Results<a class="headerlink" href="#obtaining-results" title="Permalink to this headline"></a></h2>
<p>There are two ways to obtain results:</p>
<ul class="simple">
<li><p>By providing callback in the <code class="docutils literal notranslate"><span class="pre">InferenceExectionContext</span></code> and receiving results either on host’s or NVIGI’s thread</p></li>
<li><p>By NOT providing a callback and forcing <code class="docutils literal notranslate"><span class="pre">evaluateAsync</span></code> path, which results in requiring host app to poll for result.</p></li>
</ul>
<section id="callback-approach">
<h3>Callback Approach<a class="headerlink" href="#callback-approach" title="Permalink to this headline"></a></h3>
<p>This is the simplest and easiest way to obtain results. Callback function of the following type must be provided via <code class="docutils literal notranslate"><span class="pre">InferenceExectionContext</span></code> before calling evaluate:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inferenceCallback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionContext</span><span class="o">*</span><span class="w"> </span><span class="n">execCtx</span><span class="p">,</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionState</span><span class="w"> </span><span class="n">state</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">userData</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionState</span><span class="w"> </span>
<span class="p">{</span><span class="w">     </span>
<span class="w">    </span><span class="c1">//! Optional user context to control execution </span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">userCtx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">HostProvidedCallbackCtx</span><span class="o">*</span><span class="p">)</span><span class="n">userData</span><span class="p">;</span><span class="w"> </span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">execCtx</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span><span class="w"> </span>
<span class="w">       </span><span class="k">const</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataText</span><span class="o">*</span><span class="w"> </span><span class="n">text</span><span class="p">{};</span><span class="w"> </span>
<span class="w">       </span><span class="n">execCtx</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="o">-&gt;</span><span class="n">findAndValidateSlot</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kASRDataSlotTranscribedText</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">text</span><span class="p">);</span><span class="w"> </span>
<span class="w">       </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">transcribedText</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">text</span><span class="o">-&gt;</span><span class="n">getUtf8Text</span><span class="p">();</span><span class="w"></span>
<span class="w">       </span><span class="c1">//! Do something with the received text </span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">state</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionStateDone</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span><span class="w"> </span>
<span class="w">        </span><span class="c1">//! This is all the data we can expect to receive </span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="p">(</span><span class="n">userCtx</span><span class="o">-&gt;</span><span class="n">needToInterruptInference</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span><span class="w"> </span>
<span class="w">        </span><span class="c1">//! Inform NVIGI that inference should be cancelled </span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionStateCancel</span><span class="p">;</span><span class="w"> </span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">state</span><span class="p">;</span><span class="w"> </span>
<span class="p">};</span><span class="w"> </span>

<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionContext</span><span class="w"> </span><span class="n">asrContext</span><span class="p">{};</span><span class="w"></span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">asrInstanceLocal</span><span class="p">;</span><span class="w">         </span><span class="c1">// The instance we created and we want to run inference on</span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">callback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">asrCallback</span><span class="p">;</span><span class="w">              </span><span class="c1">// Callback to receive transcribed text</span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">callbackUserData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">asrCallback</span><span class="p">;</span><span class="w">     </span><span class="c1">// Optional context for the callback, can be null if not needed</span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
<span class="c1">// BLOCKING</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">asrContext</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="n">asrContext</span><span class="p">)))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w">    </span>
</pre></div>
</div>
<blockquote>
<div><p>IMPORTANT:
To cancel inference simply return <code class="docutils literal notranslate"><span class="pre">nvigi::InferenceExecutionStateCancel</span></code> in the callback</p>
</div></blockquote>
</section>
<section id="polling-approach">
<h3>Polling Approach<a class="headerlink" href="#polling-approach" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>IMPORTANT: This is an optional way to obtain results and each individual plugin must implement special interface <code class="docutils literal notranslate"><span class="pre">nvigi::IPolledInferenceInterface</span></code> in order to enable this functionality. In addition, when using polling, <code class="docutils literal notranslate"><span class="pre">evaluateAsync</span></code> is the ONLY viable inference model since we cannot have blocking calls.</p>
</div></blockquote>
<p>Before proceeding any further it is necessary to obtain polling interface from the plugin, assuming it is actually implemented:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">nvigi</span><span class="o">::</span><span class="n">IPolledInferenceInterface</span><span class="o">*</span><span class="w"> </span><span class="n">ipolled</span><span class="p">{};</span><span class="w"></span>
<span class="n">nvigiGetInterface</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">ipolled</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Upon successful retrieval of the polling interface the next step is to skip providing callback in the execution context. This will automatically make evaluate call async (plugin will generate and manage a thread) and host application will need to check if results are ready before consuming them.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionContext</span><span class="w"> </span><span class="n">asrContext</span><span class="p">{};</span><span class="w"></span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">asrInstanceLocal</span><span class="p">;</span><span class="w">         </span><span class="c1">// The instance we created and we want to run inference on</span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">callback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w">                  </span><span class="c1">// NO CALLBACK WHEN POLLING RESULTS</span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">callbackUserData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"></span>
<span class="n">asrContext</span><span class="p">.</span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
<span class="c1">// ASYNC, note that in this mode one CANNOT use blocking evaluate call</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">asrContext</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluateAsync</span><span class="p">(</span><span class="n">asrContext</span><span class="p">)))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w">    </span>

<span class="c1">// Poll for results on host&#39;s thread</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionState</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionStateDataPending</span><span class="p">;</span><span class="w"></span>
<span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">state</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionStateDataPending</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">blocking</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// Block and wait for results</span>
<span class="w">        </span><span class="n">ipolled</span><span class="o">-&gt;</span><span class="n">getResults</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">state</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="c1">// Process results and release them</span>
<span class="w">        </span><span class="n">inferenceCallback</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">state</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="n">ipolled</span><span class="o">-&gt;</span><span class="n">releaseResults</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">state</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="k">else</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// Check if there are some results, if not move on</span>
<span class="w">        </span><span class="k">if</span><span class="p">(</span><span class="n">ipolled</span><span class="o">-&gt;</span><span class="n">getResults</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">state</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">ResultOk</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="c1">// Process results and release them</span>
<span class="w">            </span><span class="n">inferenceCallback</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">state</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">ipolled</span><span class="o">-&gt;</span><span class="n">releaseResults</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">state</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w">    </span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p>NOTE: Even with polling we still ultimately use the callback function to process output slots in the execution context, simply for convenience</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
<img src="../../_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="../../_static/NVIDIA-LogoWhite.svg" class="only-dark"/>

<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>

<p>
  Copyright &#169; 2024-2025, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Mar 13, 2025.
      </span></p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>
 



</body>
</html>