<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Generative Pre-Trained Transformers (GPT) Programming Guide &mdash; In-Game Inferencing SDK 1.0.0 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/pygments_dark.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/theme-switcher-general.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/omni-style-dark.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/api-styles-dark.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/api-styles.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/mermaid-init.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/theme-setter.js"></script>
        <script src="../../../_static/design-tabs.js"></script>
        <script src="../../../_static/version.js"></script>
        <script src="../../../_static/social-media.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Embedding (EMBED) Programming Guide" href="ProgrammingGuideEmbed.html" />
    <link rel="prev" title="Automatic Speech Recognition (ASR) - Whisper Programming Guide" href="ProgrammingGuideASRWhisper.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="../../../index.html">
  <img src="../../../_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../NVIGIDeveloperPack.html">NVIDIA In-Game Inferencing (NVIGI) Developer Pack 1.1.0 Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sample/README.html">NVIGI 3D Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nvigi_core/docs/Architecture.html">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nvigi_core/docs/GpuSchedulingForAI.html">GPU Scheduling for AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nvigi_core/docs/ProgrammingGuide.html">NVIGI - Programming Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nvigi_core/docs/ProgrammingGuideAI.html">NVIGI - Programming Guide For Local And Cloud Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">NVIDIA In-Game Inference AI Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProgrammingGuideASRWhisper.html">Automatic Speech Recognition (ASR) - Whisper Programming Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Generative Pre-Trained Transformers (GPT) Programming Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#initialize-and-shutdown">1.0 INITIALIZE AND SHUTDOWN</a></li>
<li class="toctree-l2"><a class="reference internal" href="#obtain-gpt-interface-s">2.0 OBTAIN GPT INTERFACE(S)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#create-gpt-instance-s">3.0 CREATE GPT INSTANCE(S)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#obtain-capabilities-and-requirements-for-model-s">3.1 OBTAIN CAPABILITIES AND REQUIREMENTS FOR MODEL(S)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#local">LOCAL</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cloud-vram-ignored">CLOUD (VRAM ignored)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#create-model-instance">3.2 CREATE MODEL INSTANCE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#setup-callback-to-receive-inferred-data">4.0 SETUP CALLBACK TO RECEIVE INFERRED DATA</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-the-execution-context">5.0 PREPARE THE EXECUTION CONTEXT</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#instruct-mode">5.1 INSTRUCT MODE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interactive-chat-mode">5.2 INTERACTIVE (CHAT) MODE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cloud">5.3 CLOUD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#instruct-or-chat-mode">5.3.1 INSTRUCT OR CHAT MODE</a></li>
<li class="toctree-l4"><a class="reference internal" href="#full-control-via-json-input-slot">5.3.1 FULL CONTROL VIA JSON INPUT SLOT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#outputs">5.4 OUTPUTS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#add-gpt-inference-to-the-pipeline">6.0 ADD GPT INFERENCE TO THE PIPELINE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">6.1 INSTRUCT MODE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interact-mode">6.2 INTERACT MODE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#destroy-instance-s">7.0 DESTROY INSTANCE(S)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#unload-interface-s">8.0 UNLOAD INTERFACE(S)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vlm-visual-lanuage-models">9.0 VLM (Visual Lanuage Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#appendix">APPENDIX</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#memory-tracking">MEMORY TRACKING</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cuda">CUDA</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#chat-mode-setup-summary">CHAT MODE SETUP SUMMARY</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ProgrammingGuideEmbed.html">Embedding (EMBED) Programming Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../source-build/README.html">NVIGI Public Source GitHub Pull-and-Build Scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="CustomizingPlugins.html">Creating a Customized Plugin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3rd-party-licenses.html">3rd PARTY SOFTWARE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">In-Game Inferencing SDK</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">


<li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
  
<li>Generative Pre-Trained Transformers (GPT) Programming Guide</li>

      <li class="wy-breadcrumbs-aside">
      </li>
<li class="wy-breadcrumbs-aside">

  <span>&nbsp;</span>
</li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="generative-pre-trained-transformers-gpt-programming-guide">
<h1>Generative Pre-Trained Transformers (GPT) Programming Guide<a class="headerlink" href="#generative-pre-trained-transformers-gpt-programming-guide" title="Permalink to this headline"></a></h1>
<p>The focus of this guide is on using In-Game Inferencing to integrate a GPT model into an application. One example would be <a class="reference external" href="https://llama.meta.com">Meta Llama2</a></p>
<p>Please read the <code class="docutils literal notranslate"><span class="pre">docs\ProgrammingGuideAI.md</span></code> located in the NVIGI Core package to learn more about overall AI inference API in NVIGI SDK.</p>
<blockquote>
<div><p><strong>IMPORTANT</strong>: This guide might contain pseudo code, for the up to date implementation and source code which can be copy pasted please see the <a class="reference download internal" download="" href="../../../_downloads/207224b5f077abd6af74ef46a79da9f5/basic.cpp"><span class="xref download myst">basic sample</span></a></p>
</div></blockquote>
<blockquote>
<div><p><strong>NOTE</strong> The NVIGI code currently uses the now-outdated term “General Purpose Transformer” for Generative Pre-Trained Transformers in its headers/classes/types.  This will be rectified in a coming release.</p>
</div></blockquote>
<section id="initialize-and-shutdown">
<h2>1.0 INITIALIZE AND SHUTDOWN<a class="headerlink" href="#initialize-and-shutdown" title="Permalink to this headline"></a></h2>
<p>Please read the <code class="docutils literal notranslate"><span class="pre">docs/ProgrammingGuide.md</span></code> located in the NVIGI Core package to learn more about initializing and shutting down NVIGI SDK. <a class="reference internal" href="../../../nvigi_core/docs/ProgrammingGuide.html"><span class="doc std std-doc">Which may be found here in combined binary packs</span></a></p>
</section>
<section id="obtain-gpt-interface-s">
<h2>2.0 OBTAIN GPT INTERFACE(S)<a class="headerlink" href="#obtain-gpt-interface-s" title="Permalink to this headline"></a></h2>
<p>Next, we need to retrieve GPT’s API interface based on what variant we need (cloud, CUDA, etc.).</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">nvigi</span><span class="o">::</span><span class="n">IGeneralPurposeTransformer</span><span class="w"> </span><span class="n">igptLocal</span><span class="p">{};</span><span class="w"></span>
<span class="c1">// Here we are requesting interface for the GGML_CUDA implementation</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">nvigiGetInterface</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">plugin</span><span class="o">::</span><span class="n">gpt</span><span class="o">::</span><span class="n">ggml</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">kId</span><span class="p">,</span><span class="w"> </span><span class="n">igptLocal</span><span class="p">))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">nvigi</span><span class="o">::</span><span class="n">IGeneralPurposeTransformer</span><span class="w"> </span><span class="n">igptCloud</span><span class="p">{};</span><span class="w"></span>
<span class="c1">// Here we are requesting interface for the GFN cloud implementation</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">nvigiGetInterface</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">plugin</span><span class="o">::</span><span class="n">gpt</span><span class="o">::</span><span class="n">cloud</span><span class="o">::</span><span class="n">rest</span><span class="o">::</span><span class="n">kId</span><span class="p">,</span><span class="w"> </span><span class="n">igptCloud</span><span class="p">))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p><strong>NOTE:</strong>
One can only obtain interface for a feature which is available on user system. Interfaces are valid as long as the underlying plugin is loaded and active.</p>
</div></blockquote>
</section>
<section id="create-gpt-instance-s">
<h2>3.0 CREATE GPT INSTANCE(S)<a class="headerlink" href="#create-gpt-instance-s" title="Permalink to this headline"></a></h2>
<p>Now that we have our interface we can use it to create our GPT instance. To do this, we need to provide information about GPT model we want to use, CPU/GPU resources which are available and various other creation parameters.
We can obtain this information by requesting capabilities and requirements for a model or models.</p>
<section id="obtain-capabilities-and-requirements-for-model-s">
<h3>3.1 OBTAIN CAPABILITIES AND REQUIREMENTS FOR MODEL(S)<a class="headerlink" href="#obtain-capabilities-and-requirements-for-model-s" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p><strong>IMPORTANT NOTE</strong>: This section covers a scenario where the host application can instantiate models that were not included when the application was packaged and shipped. If the models and their capabilities are predefined and there is no need for dynamically downloaded models, you can skip to the next section.</p>
</div></blockquote>
<p>There are few options here:</p>
<section id="local">
<h4>LOCAL<a class="headerlink" href="#local" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>provide specific model GUID and VRAM budget and check if that particular model can run within the budget</p></li>
<li><p>provide null model GUID and VRAM budget to get a list of models that can run within the budget</p></li>
<li><p>provide null model GUID and ‘infinite’ (SIZE_MAX) VRAM budget to get a list of ALL models</p></li>
</ul>
</section>
<section id="cloud-vram-ignored">
<h4>CLOUD (VRAM ignored)<a class="headerlink" href="#cloud-vram-ignored" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>provide specific model GUID to obtain CloudCapabilities which include URL and JSON request body for the endpoint used by the model</p></li>
<li><p>provide null model GUID to get a list of ALL models (CloudCapabilities in this case will NOT provide any info)</p></li>
</ul>
<p>Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">nvigi</span><span class="o">::</span><span class="n">CommonCreationParameters</span><span class="w"> </span><span class="n">common</span><span class="p">{};</span><span class="w"></span>
<span class="n">common</span><span class="p">.</span><span class="n">utf8PathToModels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myPathToNVIGIModelRepository</span><span class="p">;</span><span class="w"> </span><span class="c1">// Path to provided NVIGI model repository (using UTF-8 encoding)</span>
<span class="n">common</span><span class="p">.</span><span class="n">vramBudgetMB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myVRAMBudget</span><span class="p">;</span><span class="w">  </span><span class="c1">// VRAM budget (SIZE_MAX if we want ALL models) - IGNORED FOR CLOUD MODELS</span>
<span class="n">common</span><span class="p">.</span><span class="n">modelGUID</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myModelGUID</span><span class="p">;</span><span class="w"> </span><span class="c1">// Model GUID, set to `nullptr` if we want all models</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">CommonCapabilitiesAndRequirements</span><span class="o">*</span><span class="w"> </span><span class="n">caps</span><span class="p">{};</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">getCapsAndRequirements</span><span class="p">(</span><span class="n">igptLocal</span><span class="p">,</span><span class="w"> </span><span class="n">common</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">caps</span><span class="p">)))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;&#39;getCapsAndRequirements&#39; failed&quot;</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>To check if an optional feature is supported by this backend one can check if it is chained to the returned <code class="docutils literal notranslate"><span class="pre">CommonCapabilitiesAndRequirements</span></code>. Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">sampler</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">findStruct</span><span class="o">&lt;</span><span class="n">GPTSamplerParameters</span><span class="o">&gt;</span><span class="p">(</span><span class="o">*</span><span class="n">caps</span><span class="p">);</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">// This backend supports additional sampler parameters</span>
<span class="w">    </span><span class="c1">// </span>
<span class="w">    </span><span class="c1">// Set them here as desired ...</span>
<span class="w">    </span><span class="n">sampler</span><span class="o">-&gt;</span><span class="n">mirostat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="c1">// for example, use mirostat v2    </span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>In this example, the optional <code class="docutils literal notranslate"><span class="pre">GPTSamplerParameters</span></code> structure is found hence it can be populated with desired values and chained to the <code class="docutils literal notranslate"><span class="pre">GPTCreationParameter</span></code> or runtime parameters (see below for details)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">caps</span><span class="o">-&gt;</span><span class="n">numSupportedModels</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">caps</span><span class="o">-&gt;</span><span class="n">modelFlags</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">kModelFlagRequiresDownload</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// Local model, requires download</span>
<span class="w">        </span><span class="k">continue</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w">        </span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;MODEL: %s VRAM: %llu&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">caps</span><span class="o">-&gt;</span><span class="n">supportedModelNames</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">caps</span><span class="o">-&gt;</span><span class="n">modelMemoryBudgetMB</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>
<section id="create-model-instance">
<h3>3.2 CREATE MODEL INSTANCE<a class="headerlink" href="#create-model-instance" title="Permalink to this headline"></a></h3>
<p>Once we know which model we want here is an example on how to create an instance for it:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Here we are creating two instances for different backends/APIs</span>
<span class="c1">//!</span>
<span class="c1">//! IMPORTANT: This is totally optional and only used to demonstrate runtime switching between different backends</span>

<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceInstance</span><span class="o">*</span><span class="w"> </span><span class="n">gptInstanceLocal</span><span class="p">;</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">//! Creating local instance and providing our D3D12 or VK and CUDA information (all optional)</span>
<span class="w">    </span><span class="c1">//!</span>
<span class="w">    </span><span class="c1">//! This allows host to control how instance interacts with DirectX, Vulkan (if at all) or any existing CUDA contexts (if any)</span>
<span class="w">    </span><span class="c1">//!</span>
<span class="w">    </span><span class="c1">//! Note that providing DirectX/Vulkan information is mandatory if at runtime we expect instance to run on a command list.</span>

<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">CommonCreationParameters</span><span class="w"> </span><span class="n">common</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">GPTCreationParameters</span><span class="w"> </span><span class="n">params</span><span class="p">{};</span><span class="w">    </span>
</pre></div>
</div>
<p>First we need to decide how many CPU threads should our instance use (assuming selected backend provides multi-threading):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="n">common</span><span class="p">.</span><span class="n">numThreads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myNumCPUThreads</span><span class="p">;</span><span class="w"> </span><span class="c1">// How many CPU threads is instance allowed to use </span>
</pre></div>
</div>
<blockquote>
<div><p>NOTE: Spawning more than one thread when using GPU based backends might not be beneficial</p>
</div></blockquote>
<p>Next section sets the VRAM budget, this is important since some backends (like for example <code class="docutils literal notranslate"><span class="pre">ggml.cuda</span></code>) allow splitting model between CPU/GPU if there isn’t enough VRAM available. If selected backend does not provide such functionality providing insufficient VRAM budget will result in failure to create an instance.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="n">common</span><span class="p">.</span><span class="n">vramBudgetMB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myVRAMBudget</span><span class="p">;</span><span class="w">  </span><span class="c1">// How much VRAM is instance allowed to occupy</span>
</pre></div>
</div>
<p>Now we continue with setting up the rest of the creation parameters, like model repository location and model GUID:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="n">common</span><span class="p">.</span><span class="n">utf8PathToModels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myPathToNVIGIModelRepository</span><span class="p">;</span><span class="w"> </span><span class="c1">// Path to provided NVIGI model repository (using UTF-8 encoding)</span>
<span class="w">    </span><span class="n">common</span><span class="p">.</span><span class="n">modelGUID</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;{175C5C5D-E978-41AF-8F11-880D0517C524}&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// Model GUID, for details please see NVIGI models repository</span>
<span class="w">    </span><span class="n">params</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">common</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>As mention in the previous section, we detected that <code class="docutils literal notranslate"><span class="pre">GPTSamplerParameters</span></code> is supported so we can chain it with the rest of our creation paramters:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">   </span><span class="c1">// Using helper operator hence *sampler</span>
<span class="w">   </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">sampler</span><span class="p">)))</span><span class="w"></span>
<span class="w">   </span><span class="p">{</span><span class="w"></span>
<span class="w">     </span><span class="c1">// handle error</span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Next we need to provide information about D3D12 properties if our application is planning to leverage <code class="docutils literal notranslate"><span class="pre">CiG</span></code> (CUDA In Graphics)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">D3D12Parameters</span><span class="w"> </span><span class="n">d3d12Params</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">d3d12Params</span><span class="p">.</span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myDevice</span><span class="p">;</span><span class="w"> </span>
<span class="w">    </span><span class="n">d3d12Params</span><span class="p">.</span><span class="n">queue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myDirectQueue</span><span class="p">;</span><span class="w"> </span><span class="c1">// mandatory to use CIG</span>
<span class="w">    </span><span class="n">d3d12Params</span><span class="p">.</span><span class="n">queueCompute</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myComputeQueue</span><span class="p">;</span><span class="w"> </span><span class="c1">// optional</span>
<span class="w">    </span><span class="n">d3d12Params</span><span class="p">.</span><span class="n">queueCopy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myCopyQueue</span><span class="p">;</span><span class="w"> </span><span class="c1">// optional</span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">d3d12Params</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// handle error</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">igptLocal</span><span class="p">.</span><span class="n">createInstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gptInstanceLocal</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceInstance</span><span class="o">*</span><span class="w"> </span><span class="n">gptInstanceCloud</span><span class="p">;</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">CommonCreationParameters</span><span class="w"> </span><span class="n">common</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">GPTCreationParameters</span><span class="w"> </span><span class="n">params</span><span class="p">{};</span><span class="w">    </span>
<span class="w">    </span><span class="n">common</span><span class="p">.</span><span class="n">modelGUID</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;{175C5C5D-E978-41AF-8F11-880D0517C524}&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// Model GUID, for details please see NVIGI models repository</span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">common</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// handle error</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="c1">//! Cloud parameters</span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">RESTParameters</span><span class="w"> </span><span class="n">nvcfParams</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">token</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">getEnvVar</span><span class="p">(</span><span class="s">&quot;MY_TOKEN&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">token</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">nvcfParams</span><span class="p">.</span><span class="n">url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myURL</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">nvcfParams</span><span class="p">.</span><span class="n">authenticationToken</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">token</span><span class="p">.</span><span class="n">c_str</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">nvcfParams</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// handle error</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">igptCloud</span><span class="p">.</span><span class="n">createInstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gptInstanceCloud</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">countof</span><span class="p">(</span><span class="n">inputs</span><span class="p">))))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p><strong>NOTE:</strong>
NVIGI model repository is provided with the pack under <code class="docutils literal notranslate"><span class="pre">data/nvigi.models</span></code>.</p>
</div></blockquote>
</section>
</section>
<section id="setup-callback-to-receive-inferred-data">
<h2>4.0 SETUP CALLBACK TO RECEIVE INFERRED DATA<a class="headerlink" href="#setup-callback-to-receive-inferred-data" title="Permalink to this headline"></a></h2>
<p>In order to receive a text response from the GPT model inference a special callback needs to be setup like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">gptCallback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionContext</span><span class="o">*</span><span class="w"> </span><span class="n">execCtx</span><span class="p">,</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionState</span><span class="w"> </span><span class="n">state</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">userData</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionState</span><span class="w"> </span>
<span class="p">{</span><span class="w">     </span>
<span class="w">    </span><span class="c1">//! Optional user context to control execution </span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">userCtx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">HostProvidedGPTCallbackCtx</span><span class="o">*</span><span class="p">)</span><span class="n">userData</span><span class="p">;</span><span class="w"> </span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">execCtx</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span><span class="w"> </span>
<span class="w">       </span><span class="k">const</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataText</span><span class="o">*</span><span class="w"> </span><span class="n">text</span><span class="p">{};</span><span class="w">        </span>
<span class="w">       </span><span class="n">execCtx</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="o">-&gt;</span><span class="n">findAndValidateSlot</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotResponse</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">text</span><span class="p">);</span><span class="w">        </span>
<span class="w">       </span><span class="c1">//! OPTIONAL - Cloud only, REST response from the server</span>
<span class="w">       </span><span class="k">if</span><span class="p">(</span><span class="n">execCtx</span><span class="o">-&gt;</span><span class="n">instance</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">gptInstanceCloud</span><span class="p">)</span><span class="w"></span>
<span class="w">       </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="c1">// Full response from the server, normally as JSON but could be plain text in case of an error</span>
<span class="w">            </span><span class="k">const</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataText</span><span class="o">*</span><span class="w"> </span><span class="n">responseJSON</span><span class="p">{};</span><span class="w"></span>
<span class="w">            </span><span class="n">execCtx</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="o">-&gt;</span><span class="n">findAndValidateSlot</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotJSON</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">responseJSON</span><span class="p">);</span><span class="w"> </span>
<span class="w">            </span><span class="c1">// Examine response from the server</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">receivedResponse</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">responseJSON</span><span class="o">-&gt;</span><span class="n">getUtf8Text</span><span class="p">();</span><span class="w"></span>
<span class="w">       </span><span class="p">}</span><span class="w">       </span>
<span class="w">       </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">receivedAnswer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">text</span><span class="o">-&gt;</span><span class="n">getUtf8Text</span><span class="p">();</span><span class="w"></span>
<span class="w">       </span><span class="c1">//! Do something with the received answer</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">state</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">kInferenceExecutionStateDone</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span><span class="w"> </span>
<span class="w">        </span><span class="c1">//! This is all the data we can expect to receive </span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="p">(</span><span class="n">userCtx</span><span class="o">-&gt;</span><span class="n">needToInterruptInference</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="p">{</span><span class="w"> </span>
<span class="w">        </span><span class="c1">//! Inform NVIGI that inference should be cancelled </span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">nvigi</span><span class="o">::</span><span class="n">kInferenceExecutionStateCancel</span><span class="p">;</span><span class="w"> </span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">state</span><span class="p">;</span><span class="w"> </span>
<span class="p">};</span><span class="w"> </span>
</pre></div>
</div>
<blockquote>
<div><p><strong>IMPORTANT:</strong>
Input and output data slots provided within the execution context are <strong>only valid during the callback execution</strong>. Host application must be ready to handle callbacks until reaching <code class="docutils literal notranslate"><span class="pre">nvigi::InferenceExecutionStateDone</span></code> or <code class="docutils literal notranslate"><span class="pre">nvigi::InferenceExecutionStateCancel</span></code> state.</p>
</div></blockquote>
<blockquote>
<div><p><strong>NOTE:</strong>
To cancel GPT inference make sure to return <code class="docutils literal notranslate"><span class="pre">nvigi::InferenceExecutionStateCancel</span></code> state in the callback.</p>
</div></blockquote>
</section>
<section id="prepare-the-execution-context">
<h2>5.0 PREPARE THE EXECUTION CONTEXT<a class="headerlink" href="#prepare-the-execution-context" title="Permalink to this headline"></a></h2>
<p>Before GPT can be evaluated the <code class="docutils literal notranslate"><span class="pre">nvigi::InferenceExecutionContext</span></code> needs to be defined. Among other things, this includes specifying input and output slots. GPT can operate in two modes:</p>
<ul class="simple">
<li><p>Instruct a model and receive an answer.</p></li>
<li><p>Interact with a model while taking turns in a guided conversation.</p></li>
</ul>
<p>We will be using the <code class="docutils literal notranslate"><span class="pre">InferenceDataTextSTLHelper</span></code> helper class to convert text to <code class="docutils literal notranslate"><span class="pre">nvigi::InferenceDataText</span></code>:</p>
<section id="instruct-mode">
<h3>5.1 INSTRUCT MODE<a class="headerlink" href="#instruct-mode" title="Permalink to this headline"></a></h3>
<p>In this mode GPT receives <strong>any combination</strong> of <code class="docutils literal notranslate"><span class="pre">system</span></code>, <code class="docutils literal notranslate"><span class="pre">user</span></code> and <code class="docutils literal notranslate"><span class="pre">assistant</span></code> input slots with instruction(s) and produces adequate response. Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! We can provide ANY combination of input slots, it can be just $user for example.</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">system</span><span class="p">(</span><span class="s">&quot;You are a world renown and extremely talented writer.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">user</span><span class="p">(</span><span class="s">&quot;Write a poem about transformers, robots in disguise&quot;</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Using our helper from the section above</span>
<span class="n">InferenceDataTextSTLHelper</span><span class="w"> </span><span class="nf">systemSlot</span><span class="p">(</span><span class="n">system</span><span class="p">);</span><span class="w"></span>
<span class="n">InferenceDataTextSTLHelper</span><span class="w"> </span><span class="nf">userSlot</span><span class="p">(</span><span class="n">user</span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlot</span><span class="o">&gt;</span><span class="w"> </span><span class="n">slots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotSystem</span><span class="p">,</span><span class="w"> </span><span class="n">systemSlot</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotUser</span><span class="p">,</span><span class="w"> </span><span class="n">userSlot</span><span class="p">}</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlotArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="p">};</span><span class="w"> </span><span class="c1">// Input slots</span>

<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionContext</span><span class="w"> </span><span class="n">gptContext</span><span class="p">{};</span><span class="w"></span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptInstanceLocal</span><span class="p">;</span><span class="w">         </span><span class="c1">// The instance we created and we want to run inference on</span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">callback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptCallback</span><span class="p">;</span><span class="w">              </span><span class="c1">// Callback to receive transcribed text</span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">callbackUserData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gptCallbackCtx</span><span class="p">;</span><span class="w">  </span><span class="c1">// Optional context for the callback, can be null if not needed</span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="w"></span>
</pre></div>
</div>
<blockquote>
<div><p>NOTE: Prompt templating is defined in <code class="docutils literal notranslate"><span class="pre">nvigi.model.config.json</span></code> for each model</p>
</div></blockquote>
</section>
<section id="interactive-chat-mode">
<h3>5.2 INTERACTIVE (CHAT) MODE<a class="headerlink" href="#interactive-chat-mode" title="Permalink to this headline"></a></h3>
<p>In this mode, the first call to evaluate instance that includes <code class="docutils literal notranslate"><span class="pre">system</span></code> is used to setup the conversation. Consecutive calls which do NOT include <code class="docutils literal notranslate"><span class="pre">system</span></code> slot are considered a new “turn” in a conversation. Providing <code class="docutils literal notranslate"><span class="pre">system</span></code> input slot at any point in time resets conversation with new setup. Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">system</span><span class="p">(</span><span class="s">&quot;This is a transcript of a conversation between Rob and Bob. Rob enters the room.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">user</span><span class="p">(</span><span class="s">&quot;Hey Bob, how are you?&quot;</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Using our helper from the section above</span>
<span class="n">InferenceDataTextSTLHelper</span><span class="w"> </span><span class="nf">systemSlot</span><span class="p">(</span><span class="n">system</span><span class="p">);</span><span class="w"></span>
<span class="n">InferenceDataTextSTLHelper</span><span class="w"> </span><span class="nf">userSlot</span><span class="p">(</span><span class="n">user</span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlot</span><span class="o">&gt;</span><span class="w"> </span><span class="n">slots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotSystem</span><span class="p">,</span><span class="w"> </span><span class="n">systemSlot</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotUser</span><span class="p">,</span><span class="w"> </span><span class="n">userSlot</span><span class="p">}</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlotArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="p">};</span><span class="w"> </span><span class="c1">// Input slots</span>

<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceExecutionContext</span><span class="w"> </span><span class="n">gptContext</span><span class="p">{};</span><span class="w"></span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptInstanceLocal</span><span class="p">;</span><span class="w">         </span><span class="c1">// The instance we created and we want to run inference on</span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">callback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptCallback</span><span class="p">;</span><span class="w">              </span><span class="c1">// Callback to receive transcribed text</span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">callbackUserData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gptCallbackCtx</span><span class="p">;</span><span class="w">  </span><span class="c1">// Optional context for the callback, can be null if not needed</span>
<span class="n">gptContext</span><span class="p">.</span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="w"></span>
</pre></div>
</div>
<p>Further down in this document we will explain how to setup interactive mode and reverse prompt when evaluating your GPT instance.</p>
<blockquote>
<div><p>NOTE: All input slots are considered optional, any combination of inputs is acceptable but <strong>at least one has to be provided</strong>.</p>
</div></blockquote>
</section>
<section id="cloud">
<h3>5.3 CLOUD<a class="headerlink" href="#cloud" title="Permalink to this headline"></a></h3>
<p>Cloud inference can operate in two modes as described in next two sections.</p>
<section id="instruct-or-chat-mode">
<h4>5.3.1 INSTRUCT OR CHAT MODE<a class="headerlink" href="#instruct-or-chat-mode" title="Permalink to this headline"></a></h4>
<p>In this mode cloud inference is <strong>exactly the same as the local one</strong> which is described in the previous section. The only difference would be to obtain interface from <code class="docutils literal notranslate"><span class="pre">nvigi::plugin::gpt::cloud::rest::kId</span></code> plugin and create instance with it as shown in <a class="reference internal" href="#create-gpt-instance-s"><span class="std std-doc">section 3</span></a></p>
</section>
<section id="full-control-via-json-input-slot">
<h4>5.3.1 FULL CONTROL VIA JSON INPUT SLOT<a class="headerlink" href="#full-control-via-json-input-slot" title="Permalink to this headline"></a></h4>
<p>To obtain full control over the prompt engineering the standard <code class="docutils literal notranslate"><span class="pre">system/user/assistant</span></code> input slots can be omitted and instead JSON input slot is used to define the entire REST request sent to the cloud end-point. Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! When issuing REST request we need to tell the cloud endpoint what model we want and what parameters to use</span>
<span class="c1">//!</span>
<span class="c1">//! IMPORTANT: Make sure to replace $system, $user, $assistant as needed</span>
<span class="n">json</span><span class="w"> </span><span class="n">restJSONBody</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sa">R</span><span class="s">&quot;</span><span class="dl">(</span><span class="s">{</span>
<span class="s">    &quot;model&quot; : &quot;meta/llama2-70b&quot;,</span>
<span class="s">    &quot;messages&quot;: [</span>
<span class="s">        {</span>
<span class="s">            &quot;role&quot;:&quot;system&quot;,</span>
<span class="s">            &quot;content&quot;:&quot;$system&quot;</span>
<span class="s">        },</span>
<span class="s">        {</span>
<span class="s">            &quot;role&quot;:&quot;user&quot;,</span>
<span class="s">            &quot;content&quot;:&quot;$user&quot;</span>
<span class="s">        },</span>
<span class="s">        {</span>
<span class="s">            &quot;role&quot;:&quot;assistant&quot;,</span>
<span class="s">            &quot;content&quot;:&quot;$assistant&quot;</span>
<span class="s">        }</span>
<span class="s">    ],</span>
<span class="s">    &quot;stream&quot;: false,</span>
<span class="s">    &quot;temperature&quot;: 0.5,</span>
<span class="s">    &quot;top_p&quot; : 1,</span>
<span class="s">    &quot;max_tokens&quot;: 1024</span>
<span class="s">}</span><span class="dl">)</span><span class="s">&quot;</span><span class="n">_json</span><span class="p">;</span><span class="w"></span>

<span class="c1">// Creating extra input slot to provide JSON body</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">restJSONBodyAsString</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">restJSONBody</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="sc">&#39; &#39;</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">json</span><span class="o">::</span><span class="n">error_handler_t</span><span class="o">::</span><span class="n">replace</span><span class="p">);</span><span class="w"></span>
<span class="n">InferenceDataTextSTLHelper</span><span class="w"> </span><span class="nf">jsonSlot</span><span class="p">(</span><span class="n">restJSONBodyAsString</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Only providing one input slot, JSON</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlot</span><span class="o">&gt;</span><span class="w"> </span><span class="n">slots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotJSON</span><span class="p">,</span><span class="w"> </span><span class="n">jsonSlot</span><span class="p">}</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlotArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">slots</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="p">};</span><span class="w"> </span><span class="c1">// Input slots</span>
</pre></div>
</div>
</section>
</section>
<section id="outputs">
<h3>5.4 OUTPUTS<a class="headerlink" href="#outputs" title="Permalink to this headline"></a></h3>
<p>If the output slots are not provided, as in the above examples, NVIGI will allocate them. This is the simplest and recommended way, output slots will be provided via callback and will only be valid within the callback execution framework.</p>
<blockquote>
<div><p><strong>IMPORTANT:</strong>
The execution context and all provided data (input, output slots) must be valid at the time <code class="docutils literal notranslate"><span class="pre">instance-&gt;evaluate</span></code> is called (see below for more details).</p>
</div></blockquote>
</section>
</section>
<section id="add-gpt-inference-to-the-pipeline">
<h2>6.0 ADD GPT INFERENCE TO THE PIPELINE<a class="headerlink" href="#add-gpt-inference-to-the-pipeline" title="Permalink to this headline"></a></h2>
<p>In your execution pipeline, call <code class="docutils literal notranslate"><span class="pre">instance-&gt;evaluate</span></code> at the appropriate location where a prompt needs to be processed to receive a response from the GPT.</p>
<section id="id1">
<h3>6.1 INSTRUCT MODE<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>In this mode we evaluate given prompt and receive our response like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Make sure GPT is available and user selected this option in the UI</span>
<span class="k">if</span><span class="p">(</span><span class="n">useGPT</span><span class="p">)</span><span class="w"> </span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">//! OPTIONAL Runtime properties, we can for example change seed or how many tokes to predict etc.</span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">GPTRuntimeParameters</span><span class="w"> </span><span class="n">gptRuntime</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myRNGSeed</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">tokensToPredict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span><span class="w"> </span><span class="c1">// modify as needed</span>
<span class="w">    </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">interactive</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">gptContext</span><span class="p">.</span><span class="n">runtimeParameters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gptRuntime</span><span class="p">;</span><span class="w"></span>

<span class="w">    </span><span class="c1">//! OPTIONAL Runtime sampler properties, not necessarily supported by all backends</span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">GPTSamplerParameters</span><span class="w"> </span><span class="n">gptSampler</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">gptSampler</span><span class="p">.</span><span class="n">penaltyRepeat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.1f</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">gptSampler</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// handle error</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="c1">//! OPTIONAL - Switching backends at runtime (could depend on current latency, available resources etc.)</span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">useLocalInference</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptInstanceLocal</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">d3d12Runtime</span><span class="p">);</span><span class="w"> </span><span class="c1">// makes sense only for local inference</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="k">else</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptInstanceCloud</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w">    </span>
<span class="w">    </span>
<span class="w">    </span><span class="c1">// Evaluate our instance with all the additional parameters, transcribed text is received via callback    </span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="n">gptContext</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w">    </span>

<span class="w">    </span><span class="c1">//! IMPORTANT: Wait for the callback to receive nvigi::InferenceExecutionStateDone</span>

<span class="w">    </span><span class="c1">//! Now we have received the response from the GPT via our gptCallback</span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="interact-mode">
<h3>6.2 INTERACT MODE<a class="headerlink" href="#interact-mode" title="Permalink to this headline"></a></h3>
<p>In this scenario, GPT must process the prompt first and then we need to feed user input based on the responses received. Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Make sure GPT is available and user selected this option in the UI</span>
<span class="k">if</span><span class="p">(</span><span class="n">useGPT</span><span class="p">)</span><span class="w"> </span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">//! OPTIONAL Runtime properties, we can for example change seed or how many tokes to predict etc.</span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">GPTRuntimeParameters</span><span class="w"> </span><span class="n">gptRuntime</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">myRNGSeed</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">tokensToPredict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span><span class="w"> </span><span class="c1">// modify as needed</span>
<span class="w">    </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">interactive</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">     </span><span class="c1">// Set to true if prompt engineering is used to start a guided conversation</span>
<span class="w">    </span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">reversePrompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Rob:&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// This has to match our prompt, it represents the user</span>
<span class="w">    </span><span class="n">gptContext</span><span class="p">.</span><span class="n">runtimeParameters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gptRuntime</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span>
<span class="w">    </span><span class="c1">//! OPTIONAL Runtime sampler properties, not necessarily supported by all backends</span>
<span class="w">    </span><span class="n">nvigi</span><span class="o">::</span><span class="n">GPTSamplerParameters</span><span class="w"> </span><span class="n">gptSampler</span><span class="p">{};</span><span class="w"></span>
<span class="w">    </span><span class="n">gptSampler</span><span class="p">.</span><span class="n">penaltyRepeat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.1f</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">gptRuntime</span><span class="p">.</span><span class="n">chain</span><span class="p">(</span><span class="n">gptSampler</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// handle error</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>

<span class="w">    </span><span class="c1">//! OPTIONAL - Switching backends at runtime (could depend on current latency, available resources etc.)</span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">useLocalInference</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptInstanceLocal</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="k">else</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gptInstanceCloud</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w">    </span>
<span class="w">    </span>
<span class="w">    </span><span class="c1">// Process system input slot first (assuming it was set already as shown in section 5.2)</span>
<span class="w">    </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="n">gptContext</span><span class="p">)))</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w">    </span>
<span class="w">    </span><span class="k">else</span><span class="w"></span>
<span class="w">    </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">//! IMPORTANT: Wait for the callback to receive nvigi::InferenceExecutionStateDone before proceeding here</span>

<span class="w">        </span><span class="c1">// Display response from GPT on screen</span>

<span class="w">        </span><span class="c1">// Now we enter the conversation</span>
<span class="w">        </span><span class="k">while</span><span class="p">(</span><span class="n">runConversation</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="c1">// Setting up new context and new input/output slots</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getUserInputBasedOnResponseReceivedFromGPT</span><span class="p">();</span><span class="w"></span>
<span class="w">            </span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataTextSTLHelper</span><span class="w"> </span><span class="nf">userSlot</span><span class="p">(</span><span class="n">input</span><span class="p">);</span><span class="w">        </span>
<span class="w">            </span><span class="c1">// Note that here we are providing `user input` slot and no system</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlot</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inSlots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotUser</span><span class="p">,</span><span class="w"> </span><span class="n">userSlot</span><span class="p">}</span><span class="w"> </span><span class="p">};</span><span class="w">        </span>
<span class="w">            </span><span class="n">InferenceDataSlotArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">inSlots</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">inSlots</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="w">            </span><span class="n">InferenceDataSlotArray</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">outSlots</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">outSlots</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="p">};</span><span class="w"></span>

<span class="w">            </span><span class="n">gptContext</span><span class="p">.</span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">;</span><span class="w"></span>
<span class="w">            </span><span class="n">gptContext</span><span class="p">.</span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">;</span><span class="w">        </span>
<span class="w">            </span>
<span class="w">            </span><span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="n">gptContext</span><span class="p">)))</span><span class="w"></span>
<span class="w">            </span><span class="p">{</span><span class="w"></span>
<span class="w">                </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="p">}</span><span class="w"></span>

<span class="w">            </span><span class="c1">//! IMPORTANT: Wait for the callback to receive nvigi::InferenceExecutionStateDone before proceeding</span>

<span class="w">            </span><span class="c1">// Display response from GPT on screen</span>
<span class="w">        </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w">    </span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>To start a new conversation <strong>simply evaluate your instance with the new SYSTEM input slot</strong> and then repeat the same process.</p>
</section>
</section>
<section id="destroy-instance-s">
<h2>7.0 DESTROY INSTANCE(S)<a class="headerlink" href="#destroy-instance-s" title="Permalink to this headline"></a></h2>
<p>Once GPT is no longer needed each instance should be destroyed like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Finally, we destroy our instance(s)</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">igptLocal</span><span class="p">.</span><span class="n">destroyInstance</span><span class="p">(</span><span class="n">gptInstanceLocal</span><span class="p">)))</span><span class="w"> </span>
<span class="p">{</span><span class="w"> </span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">igptCloud</span><span class="p">.</span><span class="n">destroyInstance</span><span class="p">(</span><span class="n">gptInstanceCloud</span><span class="p">)))</span><span class="w"> </span>
<span class="p">{</span><span class="w"> </span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span>
</pre></div>
</div>
</section>
<section id="unload-interface-s">
<h2>8.0 UNLOAD INTERFACE(S)<a class="headerlink" href="#unload-interface-s" title="Permalink to this headline"></a></h2>
<p>Once GPT is no longer needed each interface should be unloaded like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">//! Finally, we destroy our instance(s)</span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">nvigiUnloadInterface</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">plugin</span><span class="o">::</span><span class="n">gpt</span><span class="o">::</span><span class="n">cloud</span><span class="o">::</span><span class="n">rest</span><span class="o">::</span><span class="n">kId</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">igptCloud</span><span class="p">)))</span><span class="w"> </span>
<span class="p">{</span><span class="w"> </span>
<span class="w">    </span><span class="c1">//! Check error</span>
<span class="p">}</span><span class="w"></span>
<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="w"> </span><span class="n">nvigiUnloadInterface</span><span class="p">(</span><span class="n">nvigi</span><span class="o">::</span><span class="n">plugin</span><span class="o">::</span><span class="n">gpt</span><span class="o">::</span><span class="n">ggml</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">kId</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">igptLocal</span><span class="p">)))</span><span class="w"> </span>
<span class="p">{</span><span class="w"> </span>
<span class="w">    </span><span class="c1">//! Check error</span>
<span class="p">}</span><span class="w"> </span>
</pre></div>
</div>
</section>
<section id="vlm-visual-lanuage-models">
<h2>9.0 VLM (Visual Lanuage Models)<a class="headerlink" href="#vlm-visual-lanuage-models" title="Permalink to this headline"></a></h2>
<p>The GPT plugin can also load some VILA based VLM models.  These models allows the prompts to discuss input images.
The setup and usage is the same as a standard LLM model.  For the input InferenceDataSlot, you may now also pass in an InferenceDataImage,
currently limited to one image per prompt.  The input image should be a byte array in RGB format, 8 bits per channel.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">prompt</span><span class="p">(</span><span class="w"> </span><span class="s">&quot;Describe this picture&quot;</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">my_image</span><span class="p">(</span><span class="s">&quot;picture.jpg&quot;</span><span class="p">);</span><span class="w"></span>
<span class="kt">int</span><span class="w"> </span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">;</span><span class="w"></span>
<span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">rgb_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">stbi_load</span><span class="p">(</span><span class="n">my_image</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span><span class="w"></span>
<span class="p">...</span><span class="w"></span>
<span class="c1">// prompt </span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">CpuData</span><span class="w"> </span><span class="n">text</span><span class="p">(</span><span class="n">prompt</span><span class="p">.</span><span class="n">length</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">prompt</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataText</span><span class="w"> </span><span class="nf">prompt_data</span><span class="p">(</span><span class="n">text</span><span class="p">);</span><span class="w"></span>

<span class="c1">// image</span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">CpuData</span><span class="w"> </span><span class="nf">image</span><span class="p">(</span><span class="n">h</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">rgb_data</span><span class="p">);</span><span class="w"></span>
<span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataImage</span><span class="w"> </span><span class="nf">image_data</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="p">,</span><span class="w"> </span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">);</span><span class="w"></span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">nvigi</span><span class="o">::</span><span class="n">InferenceDataSlot</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inSlots</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotUser</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">prompt_data</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="n">nvigi</span><span class="o">::</span><span class="n">kGPTDataSlotImage</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">image_data</span><span class="p">}</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="n">InferenceDataSlotArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">inSlots</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">inSlots</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="p">};</span><span class="w"> </span><span class="c1">// Input slots</span>
<span class="p">...</span><span class="w"></span>

<span class="k">if</span><span class="p">(</span><span class="n">NVIGI_FAILED</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="w"> </span><span class="n">gptContext</span><span class="p">.</span><span class="n">instance</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(</span><span class="n">gptContext</span><span class="p">)))</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">LOG</span><span class="p">(</span><span class="s">&quot;NVIGI call failed, code %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">res</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w">   </span>

<span class="p">...</span><span class="w"></span>
<span class="c1">// handle output response text from VLM same as a standard LLM</span>
<span class="p">...</span><span class="w"></span>
<span class="n">stbi_image_free</span><span class="p">(</span><span class="n">rgb_data</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>If no image is passed in, the VLM will still function as a typical LLM.</p>
</section>
<section id="appendix">
<h2>APPENDIX<a class="headerlink" href="#appendix" title="Permalink to this headline"></a></h2>
<section id="memory-tracking">
<h3>MEMORY TRACKING<a class="headerlink" href="#memory-tracking" title="Permalink to this headline"></a></h3>
<section id="cuda">
<h4>CUDA<a class="headerlink" href="#cuda" title="Permalink to this headline"></a></h4>
<p>NVIGI provides callback mechanism to track/allocated/free GPU resources as defined in the <code class="docutils literal notranslate"><span class="pre">nvigi_cuda.h</span></code> header. Here is an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Callback implementations</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">MallocReportCallback</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">user_context</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">user_context</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Malloc Report: Allocated &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; bytes at &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; (User context value: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kt">void</span><span class="w"> </span><span class="nf">FreeReportCallback</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">user_context</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">user_context</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Free Report: Freed memory at &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; (User context value: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kt">int32_t</span><span class="w"> </span><span class="nf">MallocCallback</span><span class="p">(</span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">managed</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">hip</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">user_context</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">user_context</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="o">*</span><span class="n">ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span><span class="w"> </span><span class="c1">// Simulate CUDA malloc</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">ptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Malloc Callback: Allocated &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; bytes on device &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">device</span><span class="w"> </span>
<span class="w">                  </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; (Managed: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">managed</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;, HIP: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">hip</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;, Context: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// Success</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// Failure</span>
<span class="p">}</span><span class="w"></span>

<span class="kt">int32_t</span><span class="w"> </span><span class="nf">FreeCallback</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">user_context</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">user_context</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">free</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span><span class="w"> </span><span class="c1">// Simulate CUDA free</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Free Callback: Freed memory at &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span>
<span class="w">                  </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; (User context value: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// Success</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// Failure</span>
<span class="p">}</span><span class="w"></span>

<span class="c1">// Example usage</span>
<span class="n">CudaParameters</span><span class="w"> </span><span class="n">params</span><span class="p">{};</span><span class="w"></span>

<span class="c1">// User context for tracking (e.g., an integer counter)</span>
<span class="kt">int</span><span class="w"> </span><span class="n">userContextValue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">42</span><span class="p">;</span><span class="w"></span>

<span class="c1">// Set up callbacks</span>
<span class="n">params</span><span class="p">.</span><span class="n">cudaMallocReportCallback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MallocReportCallback</span><span class="p">;</span><span class="w"></span>
<span class="n">params</span><span class="p">.</span><span class="n">cudaMallocReportUserContext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">userContextValue</span><span class="p">;</span><span class="w"></span>

<span class="n">params</span><span class="p">.</span><span class="n">cudaFreeReportCallback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FreeReportCallback</span><span class="p">;</span><span class="w"></span>
<span class="n">params</span><span class="p">.</span><span class="n">cudaFreeReportUserContext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">userContextValue</span><span class="p">;</span><span class="w"></span>

<span class="n">params</span><span class="p">.</span><span class="n">cudaMallocCallback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MallocCallback</span><span class="p">;</span><span class="w"></span>
<span class="n">params</span><span class="p">.</span><span class="n">cudaMallocUserContext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">userContextValue</span><span class="p">;</span><span class="w"></span>

<span class="n">params</span><span class="p">.</span><span class="n">cudaFreeCallback</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FreeCallback</span><span class="p">;</span><span class="w"></span>
<span class="n">params</span><span class="p">.</span><span class="n">cudaFreeUserContext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">userContextValue</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>
<section id="chat-mode-setup-summary">
<h3>CHAT MODE SETUP SUMMARY<a class="headerlink" href="#chat-mode-setup-summary" title="Permalink to this headline"></a></h3>
<p>Here is the summary of the steps needed to setup the interactive (chat) mode:</p>
<ol class="arabic simple">
<li><p>Provide <code class="docutils literal notranslate"><span class="pre">nvigi::GPTRuntimeParameters</span></code> in the execution context and set <code class="docutils literal notranslate"><span class="pre">interactive</span></code> flag to TRUE</p></li>
<li><p>Setup the conversation context by providing <code class="docutils literal notranslate"><span class="pre">nvigi::kGPTDataSlotSystem</span></code> input slot</p></li>
<li><p>Wait until callback returns <code class="docutils literal notranslate"><span class="pre">nvigi::kInferenceExecutionStateDone</span></code></p></li>
<li><p>Get user’s input and take turn in the conversation by providing <code class="docutils literal notranslate"><span class="pre">nvigi::kGPTDataSlotUser</span></code> input slot</p></li>
<li><p>Wait until callback returns <code class="docutils literal notranslate"><span class="pre">nvigi::kInferenceExecutionStateDone</span></code> and full response is provided by the plugin</p></li>
<li><p>Inform user about the response</p></li>
<li><p>To continue conversation go back to the step #4</p></li>
<li><p>To start a new conversation, go to the step #2 and setup new conversation context</p></li>
</ol>
<blockquote>
<div><p>NOTE: When changing models there is no need to change any of the setup above, only model GUID would be different when creating GPT instance</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
<img src="../../../_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="../../../_static/NVIDIA-LogoWhite.svg" class="only-dark"/>

<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>

<p>
  Copyright &#169; 2024-2025, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Mar 13, 2025.
      </span></p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>
 



</body>
</html>